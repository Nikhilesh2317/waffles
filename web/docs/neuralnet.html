<html>
<head>
	<style>
		.code {
			margin-left: 30px;
			color:#000000;
			background-color:#ffffff;
		}
	</style>
</head>
<body bgcolor=#d0d0a0><br><br><br><br>
<table align=center cellpadding=50 border=1 bgcolor=#e0e0d0 width=1000><tr><td>
<a href="../index.html#toc">Back to the table of contents</a><br>

<br>
<a href="bayesnet.html">Previous</a>&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;<a href="bugs.html">Next</a>







<h2>Neural network examples</h2>
<p>This document gives examples for writing code to use the neural network class in Waffles. Let's begin with an overview of the most important classes:
<br>
<center><img src="nndiagram.svg" width=400></center><br>
<ul>
	<li>Some traditional textbooks consider each layer in a neural network to contain weights and a nonlinearity (a.k.a. activation function, a.k.a. transfer function).
		To enable greater flexibility, we treat these as two separate blocks of network units.</li>
	<li>Each <b>GBlock</b> object represents a block of network units.
		There are many types of blocks: GBlockLinear contains a fully-connected set of weights. GBlockTanh contains tanh units. Etc.</li>
	<li>A <b>GNeuralNet</b> contains many GBlock objects, organized into layers.</li>
	<li>You can call GNeuralNet::add to add a block to your neural network. It will be added on the output-most end as a new layer.</li>
	<li>Alternatively, you can call GNeuralNet::concatenate to append a block to the last existing layer.</li>
	<li>GNeuralNet is also a type of GBlock, so you can nest entire neural networks inside of neural networks if you want to.</li>
	<li><b>GNeuralNetOptimizer</b> is the base class for methods that train the weights of a GNeuralNet.
		There are several types of GNeuralNetOptimizer. For example, GSGDOptimizer trains the neural net with stochastic gradient descent.</li>
	<li><b>GNeuralNetContext</b> holds all of the buffers that a thread needs to train or evaluate a GNeuralNet in a thread-safe manner.
		(For example it enables two threads to simultaneously feed a vector through the neural net without stomping over the activations of the other thread.)</li>
	<li><b>GNeuralNetLearner</b> is a wrapper around GNeuralNet that implements GIncrementalLearner (which implements GSupervisedLearner).
		This class enables using a GNeuralNet in place of any other learning algorithm in Waffles.</li>
</ul>

</p>



<br><br>
<h3>Logistic regression</h3>

<p>Logistic regression is fitting your data with a single layer of logistic units. Here are the #includes that we are going to need for this example:</p>

<pre class="code">
	#include &lt;GClasses/GActivation.h&gt;
	#include &lt;GClasses/GHolders.h&gt;
	#include &lt;GClasses/GMatrix.h&gt;
	#include &lt;GClasses/GNeuralNet.h&gt;
	#include &lt;GClasses/GRand.h&gt;
	#include &lt;GClasses/GTransform.h&gt;
</pre>

<p>We are going to need some data, so let's load some data from an ARFF file. Let's use <a href="http://mldata.org/repository/data/download/arff/datasets-uci-iris/">Iris</a>,
a well-known dataset for machine learning examples:</p>

<pre class="code">
	GMatrix data;
	data.loadArff("iris.arff");
</pre>

<p>"data" is a 150x5 matrix. Next, we need to divide this data into a feature matrix (the inputs) and a label matrix (the outputs):</p>

<pre class="code">
	GDataColSplitter cs(data, 1); // the "iris" dataset has only 1 column of "labels"
	const GMatrix&amp; inputs = cs.features();
	const GMatrix&amp; outputs = cs.labels();
</pre>

<p>"inputs" is a 150x4 matrix of real values, and "outputs" is a 150x1 matrix of categorical values.
Neural networks typically only support continuous values, but the labels in the iris dataset are categorical, so we will convert them to use a real-valued representation
(also known as a categorical distribution, a one-hot representation, or binarized representation):</p>

<pre class="code">
	GNominalToCat nc;
	nc.train(outputs);
	GMatrix* pRealOutputs = nc.transformBatch(outputs);
</pre>

<p>pRealOutputs points to a 150x3 matrix of real values. Now, lets further divide our data into a training portion and a testing portion:</p>

<pre class="code">
	GRand r(0);
	GDataRowSplitter rs(inputs, *pRealOutputs, r, 75);
	const GMatrix&amp; trainingFeatures = rs.features1();
	const GMatrix&amp; trainingLabels = rs.labels1();
	const GMatrix&amp; testingFeatures = rs.features2();
	const GMatrix&amp; testingLabels = rs.labels2();
</pre>

Now, we are ready to train a layer of logistic units that takes 4 inputs and gives 3 outputs.
The activation function is specified as a separate layer:</p>

<pre class="code">
	GNeuralNet nn;
	nn.add(new GBlockLinear(3));
	nn.add(new GBlockLogistic());
</pre>

To train our model, we will create an optimizer. We wil use stochastic gradient descent (SGD). We also set the learning rate here:

<pre class="code">
	GRand(0);
	GSGDOptimizer optimizer(nn, rand);
	optimizer.setLearningRate(0.05);
	optimizer.optimizeWithValidation(trainingFeatures, trainingLabels);
</pre>

<p>Let's test our model to see how well it performs:</p>

<pre class="code">
	double sse = nn.measureLoss(testingFeatures, testingLabels);
	double mse = sse / testingLabels.rows();
	double rmse = sqrt(mse);
	std::cout &lt;&lt; "The root-mean-squared test error is " &lt;&lt; to_str(rmse) &lt;&lt; "\n";
</pre>

<p>Finally, don't forget to delete pRealOutputs:</p>

<pre class="code">
	delete(pRealOutputs);
</pre>

<p>Or, preferably:</p>

<pre class="code">
	std::unique_ptr&lt;GMatrix&gt; hRealOutputs(pRealOutputs);
</pre>




<br><br>
<h3>Classification</h3>

<p>The previous example was not actually very useful because root-mean-squared only tells us how poorly the neural network fit to our
continuous representation of the data. It does not really tell us how accurate the neural network is for classifying this data.
So, instead of transforming the data to meet the model, we can transform the model to meet the data. Specifically,
we can use the GAutoFilter class to turn the neural network into a classifier:</p>

<pre class="code">
	GNeuralNetLearner learner;
	learner.nn().add(new GBlockLinear(3));
	learner.nn().add(new GBlockLogistic());
	GAutoFilter af(&amp;learner, false); // false means the auto-filter does not need to "delete(&amp;nn)" when it is destroyed.
</pre>

<p>Now, we can train the auto-filter using the original data (with nominal labels). We no longer need to explicitly train
the neural network because when we train the auto-filter, it trains the inner model.</p>

<pre class="code">
	af.train(inputs, outputs);
</pre>

<p>The auto-filter automatically filters the data as needed for its inner model, but ultimately behaves as a model that can
handle whatever types of data you have available. In this case, it turns a neural network into a classifier, since "outputs"
contains 1 column of categorical values. So, now we can obtain the misclassification rate as follows:</p>

<pre class="code">
	double mis = af.sumSquaredError(inputs, outputs);
	std::cout &lt;&lt; "The model misclassified " &lt;&lt; to_str(mis)  &lt;&lt;
		" out of " &lt;&lt; to_str(outputs.rows()) &lt;&lt; " instances.\n";
</pre>

<p>Why does a method named "sumSquaredError" return the total number of misclassifications? Because it uses Hamming distance for
categorical labels, which reports a squared-error of 1 for each misclassification.</p>

<p>(Note that if you are training a big network with big data, then efficiency may be critical. In such cases, it is generally better
to use the approach of transforming the data to meet the model, so that it does not waste a lot of time transformaing data during
training.)</p>



<br><br>
<h3>Adding layers</h3>

<p>Layers are added in feed-forward order. The first layer added is the input layer.
The last layer added is the output layer.
It is common to alternate between layers with blocks that have weights (such as GBlockLinear) and a layer with blocks that introduce non-linearity (such as GBlockTanh).
Example:</p>

<pre class="code">
	nn.add(new GBlockLinear(1000));
	nn.add(new GBlockTanh);
	nn.add(new GBlockLinear(300));
	nn.add(new GBlockTanh);
	nn.add(new GBlockLinear(90));
	nn.add(new GBlockTanh);
	nn.add(new GBlockLinear(10));
	nn.add(new GBlockTanh);
</pre>

<p>The layers may be resized as needed when the enclosing neural network is resized to fit the training data.</p>

<p>A GBlockLinear just produces linear combinations of its inputs with no activation function.
To use an activation function, add a layer with some nonlinear block. Some examples include:
GBlockRectifiedLinear, GBlockSoftPlus, GBlockGaussian, GBlockBentIdentity.</p>


<br><br>
<h3>Blocks with weights</h3>

<p>There are several types of weighted blocks that you can use in your neural networks. These include:
<ul>
	<li><b>GBlockLinear</b> - A traditional fully-connected feed-forward block of network units.</li>
	<li><b>GBlockRestrictedBoltzmannMachine</b>: A restricted boltzmann machine block.</li>
	<li><b>GBlockConvolutional1D</b> - A 1-dimensional block of convolutional units.</li>
	<li><b>GBlockConvolutional2D</b> - A 2-dimensional block of convolutional units.</li>
	<li>(Other layer types are currently under development...)</li>
</ul>



<br><br>
<h3>Stopping criteria</h3>

<p>The GDifferentiableOptimizer::optimizeWithValidation method divides the training data into a training portion and a validation
portion. The default uses 65% for training and 35% for validation. Suppose you wanted to change this
ratio to 60/40. This would be done as follows:</p>

<pre class="code">
	optimizer.optimizeWithValidation(features, labels, 0.4);
</pre>

<p>By default, training continues until validation accuracy does not improve by 0.2% over a window
of 100 epochs. If you wanted to change this to 0.1% over a window of 10 epochs, then you could do this prior to calling optimizeWithValidation:

<pre class="code">
	optimizer.setImprovementThresh(0.001);
	optimizer.setWindowSize(10);
</pre>

You can also train for a set number of epochs instead of using validation. For example, to optimize for 1000 epochs:

<pre class="code">
	optimizer.setEpochs(1000);
	optimizer.optimize(features, labels);
</pre>

By default, optimize will run stochastically through the entire set of training samples each epoch. To use a mini-match instead, set the batch size and (optionally) the number of batches per epoch before calling optimize:

<pre class="code">
	optimizer.setBatchSize(25);
	optimizer.setBatchesPerEpoch(4);
</pre>


<br><br>
<h3>Training incrementally</h3>

<p>Sometimes, it is preferable to train your neural network incrementally, instead of simply calling the "optimizeWithValidation" method.
For example, you might want to use a custom stopping criteria, you might want to report validation accuracy before
each training epoch, you might want to decay the learning rate in a particular manner, etc. The following example
shows how such things can be implemented:

<pre class="code">
	nn.beginIncrementalLearning(trainingFeatures.relation(), trainingLabels.relation());
	GRandomIndexIterator ii(trainingFeatures.rows(), nn.rand());
	for(size_t epoch = 0; epoch &lt; total_epochs; epoch++)
	{
		// Report validation accuracy
		double rmse = sqrt(nn1.sumSquaredError(validateFeatures, validateLabels) / validateLabels.rows());
		std::cout &lt;&lt; to_str(rmse) &lt;&lt; "\n";
		std::cout.flush();
	
		// Train
		ii.reset();
		size_t index;
		while(ii.next(index))
		{
			optimizer.optimizeIncremental(trainingFeatures[index], trainingLabels[index]);
		}
	
		// Decay the learning rate
		optimizer.setLearningRate(optimizer.learningRate() * 0.98);
	}
</pre>





<br><br>
<h3>Serialization</h3>

<p>You can write your neural network to a file:</p>

<pre class="code">
GDom doc;
doc.setRoot(nn.serialize(&amp;doc));
doc.saveJson("my_neural_net.json");
</pre>

<p>Then, you can load it from the file again:</p>

<pre class="code">
GDom doc;
doc.loadJson("my_neural_net.json");
GLearnerLoader ll;
GNeuralNet* pNN = (GNeuralNet*)ll.loadLearner(doc.root());
</pre>



<br><br>
<h3>MNIST</h3>

<p>A popular test for a neural network is the MNIST dataset.
(Click here to download <a href="http://uaf46365.ddns.uark.edu/data/mnist/">the data</a>.)
And, here is some code that trains a neural network with this data:</p>

<pre class="code">
#include &lt;iostream&gt;
#include &lt;cmath&gt;
#include &lt;GClasses/GApp.h&gt;
#include &lt;GClasses/GError.h&gt;
#include &lt;GClasses/GNeuralNet.h&gt;
#include &lt;GClasses/GActivation.h&gt;
#include &lt;GClasses/GTransform.h&gt;
#include &lt;GClasses/GVec.h&gt;
#include &lt;GClasses/GHolders.h&gt;

using namespace GClasses;
using std::cerr;
using std::cout;

int main(int argc, char *argv[])
{
	// Load the data
	GMatrix train;
	train.loadArff("/somepath/data/mnist/train.arff");
	GMatrix test;
	test.loadArff("/somepath/data/mnist/test.arff");
	GMatrix rawTestLabels(test, 0, test.cols() - 1, test.rows(), 1);

	// Preprocess the data
	GDataPreprocessor dpFeat(train,
		0, 0, // rowStart, colStart
		train.rows(), train.cols() - 1, // rowCount, colCount
		false, false, true, // allowMissing, allowNominal, allowContinuous
		-1.0, 1.0); // minVal, maxVal
	dpFeat.add(test, 0, 0, test.rows(), test.cols() - 1);
	GDataPreprocessor dpLab(train,
		0, train.cols() - 1, // rowStart, colStart
		train.rows(), 1, // rowCount, colCount
		false, false, true, // allowMissing, allowNominal, allowContinuous
		-1.0, 1.0); // minVal, maxVal
	dpLab.add(test, 0, test.cols() - 1, test.rows(), 1);
	GMatrix&amp; trainFeatures = dpFeat.get(0);
	GMatrix&amp; trainLabels = dpLab.get(0);
	GMatrix&amp; testFeatures = dpFeat.get(1);
	GMatrix&amp; testLabels = dpLab.get(1);

	// Make a neural network
	GNeuralNet nn;
	nn.add( new GBlockLinear(80), new GBlockTanh(),
		new GBlockLinear(30), new GBlockTanh(),
		new GBlockLinear(10), new GBlockTanh());

	// Prepare for training
	GRand rand(0);
	GSGDOptimizer optimizer(nn, rand);
	optimizer.setLearningRate(0.01);
	nn.init(trainFeatures.cols(), trainLabels.cols(), rand);
	cout &lt;&lt; "% Training patterns: " &lt;&lt; to_str(trainFeatures.rows()) &lt;&lt; "\n";
	cout &lt;&lt; "% Testing patterns: " &lt;&lt; to_str(testFeatures.rows()) &lt;&lt; "\n";
	cout &lt;&lt; "% Topology:\n";
	cout &lt;&lt; nn.to_str("% ") &lt;&lt; "\n";
	cout &lt;&lt; "@RELATION neural_net_training\n";
	cout &lt;&lt; "@ATTRIBUTE internal_rmse_train real\n";
	cout &lt;&lt; "@ATTRIBUTE internal_rmse_test real\n";
	cout &lt;&lt; "@ATTRIBUTE misclassification_rate real\n";
	cout &lt;&lt; "@DATA\n";

	// Train
	GRandomIndexIterator ii(trainFeatures.rows(), rand);
	for(size_t epoch = 0; epoch &lt; 10; epoch++)
	{
		// Validate
		double sseTrain = nn.measureLoss(trainFeatures, trainLabels);
		cout &lt;&lt; to_str(std::sqrt(sseTrain / trainFeatures.rows())) &lt;&lt; ", ";
		double sseTest = nn.measureLoss(testFeatures, testLabels);
		cout &lt;&lt; to_str(std::sqrt(sseTest / testFeatures.rows())) &lt;&lt; ", ";
		double mis = nn.measureLoss(testFeatures, rawTestLabels);
		cout &lt;&lt; to_str((double)mis / testFeatures.rows()) &lt;&lt; "\n";
		cout.flush();

		// Do an epoch of training
		ii.reset();
		size_t index;
		while(ii.next(index))
			optimizer.optimizeIncremental(trainFeatures[index], trainLabels[index]);
	}
	return 0;
}
</pre>

<p>Here are the results that I get:</p>

<pre class="code">
% Training patterns: 60000
% Testing patterns: 10000
% Topology: 784 -> 80 -> 30 -> 10
% Total weights: 65540
@RELATION neural_net_training
@ATTRIBUTE internal_rmse_train real
@ATTRIBUTE internal_rmse_test real
@ATTRIBUTE misclassification_rate real
@DATA
1.0062320115962, 1.0062882869588, 0.9235
0.35827917057482, 0.35285602819834, 0.0588
0.29920038633219, 0.30503380076365, 0.0455
0.29897590956784, 0.30469283251474, 0.0436
0.27005888129929, 0.28255127487034, 0.0382
0.26209564354629, 0.28000584880966, 0.039
0.25383945740352, 0.27075626507427, 0.0361
0.23419057786692, 0.25915069511568, 0.0335
0.23568715771943, 0.26541501669647, 0.036
0.23326108174385, 0.26323053581819, 0.0332
</pre>

<p>The right-most column shows that we get 332 misclassifications after about 2 minutes of training.
You can get much better accuracy using bigger layers, but then training will take longer too.</p>






<br><br>
<h3>Training more directly</h3>

<p>If you want more fine-grained control, you can train your neural network manually, instead of using one of the pre-built optimizers.
Here are some changes to the training section of the previous example to train the neural network with more direct calls:</p>

<pre class="code">
	// Train
<font color="red">	double learningRate = 0.01;
	double momentum = 0.0;
	GVec prediction(trainLabels.cols());
	GVec blame(trainLabels.cols());
	GVec gradient(nn.weightCount());
	gradient.fill(0.0);
	GContextNeuralNet* pCtx = nn.newContext(rand);
	std::unique_ptr&lt;GContextNeuralNet&gt; hCtx(pCtx);</font>
	GRandomIndexIterator ii(trainFeatures.rows(), rand);
	for(size_t epoch = 0; epoch &lt; 10; epoch++)
	{
		// Validate
		double sseTrain = nn.measureLoss(trainFeatures, trainLabels);
		cout &lt;&lt; to_str(std::sqrt(sseTrain / trainFeatures.rows())) &lt;&lt; ", ";
		double sseTest = nn.measureLoss(testFeatures, testLabels);
		cout &lt;&lt; to_str(std::sqrt(sseTest / testFeatures.rows())) &lt;&lt; ", ";
		double mis = nn.measureLoss(testFeatures, rawTestLabels);
		cout &lt;&lt; to_str((double)mis / testFeatures.rows()) &lt;&lt; "\n";
		cout.flush();

		// Do an epoch of training
		ii.reset();
		size_t index;
		while(ii.next(index))
<font color="red">		{
			nn.forwardProp(*pCtx, trainFeatures[index], prediction);
			blame.copy(trainLabels[index]);
			blame -= prediction;
			nn.backProp(*pCtx, trainFeatures[index], prediction, blame, blame);
			gradient *= momentum;
			nn.updateGradient(*pCtx, trainFeatures[index], blame, gradient);
			nn.step(learningRate, gradient);
		}</font>
	}
	return 0;
</pre>

<p>If you want to get more direct than that, you will probably have to start digging into the code itself.
I have worked hard to keep the code clean and easy to read, but I would welcome suggestions for improving it.</p>






<br>
<a href="bayesnet.html">Previous</a>&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;<a href="bugs.html">Next</a>

<br><br><a href="../index.html#toc">Back to the table of contents</a><br>
</td></tr></table>
</td></tr></table><br><br><br><br><br>
</body></html>
